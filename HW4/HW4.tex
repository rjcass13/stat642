\documentclass{article}
\usepackage{amsmath, amssymb, cancel, mathtools, bm}
\usepackage[left=.5in, right=.5in, top=1in, bottom=1in]{geometry}
\usepackage[most]{tcolorbox}
\usepackage[skip=1em,indent=0pt]{parskip}
\setlength{\parindent}{0pt}
\newcommand{\statvec}[1]{\underset{\sim}{\bm{#1}}} % Vector symbol (tilde under vector)
\DeclareMathOperator{\EX}{\mathbb{E}} % Expected Value symbol
\newcommand{\indep}{\perp\!\!\!\!\perp} % Independence symbol
\newcommand{\real}{\mathbb{R}} % Simplified 'Reals' indicator
\newcommand{\pto}{\overset{P}{\to}}
\newcommand{\asto}{\overset{a.s.}{\to}}
\newcommand{\rthto}{\overset{L^r}{\to}}
\newcommand{\dto}{\overset{D}{\to}}
% Force all aggregate symbols to always put values above/below
\let\Oldint=\int
\let\Oldsum=\sum
\let\Oldprod=\prod
\let\Oldbigcup=\bigcup
\let\Oldbigcap=\bigcap
\let\Oldlim=\lim
\renewcommand{\int}{\Oldint\limits} 
\renewcommand{\sum}{\Oldsum\limits} 
\renewcommand{\prod}{\Oldprod\limits} 
\renewcommand{\bigcup}{\Oldbigcup\limits} 
\renewcommand{\bigcap}{\Oldbigcap\limits} 
\renewcommand{\lim}{\Oldlim\limits} 
\newcommand{\infint}{\int_{-\infty}^{\infty}}
\newcommand{\iiddist}{\overset{\mathrm{iid}}{\sim}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\boldred}[1]{\textbf{\textcolor{red}{#1}}}


\begin{document}

\section{1}

$X_i \iiddist Pois(\lambda), \lambda > 0$. Use only the definition of sufficiency to prove that $T(X) = \sum_{i=1}^n X_i$ is sufficient for $\lambda$

\underline{Class Definition}: A statistic $T$ is sufficient if the distribution of $X|T$ does not depend on $\theta$.

Because the $X_i$s are independent, the sum is also Poisson: $T(X) \sim Pois(n \lambda)$

$P(\statvec{X} = \statvec{x}|T(X)) = \frac{P(\statvec{X} = \statvec{x})}{P(T(X) = t)} = \frac{\prod \frac{e^{-\lambda}\lambda^{x_i}}{x_i!}}{\frac{e^{-n\lambda}(n\lambda)^t}{t!}} = \frac{e^{-n\lambda}\lambda^{\sum x_i}t!}{\prod x_i! e^{-n\lambda} n^t \lambda^t}$

We defined above that in this case, $t = \sum x_i$, otherwise the above is zero/doesn't mean anything:

$\therefore = \frac{\lambda^{\sum x_i}t!}{\prod x_i! n^t \lambda^{\sum x_i}} = \underline{\frac{t!}{\prod x_i! n^{\sum x_i}}}$

That final expression does not depend on $\lambda$, thus $T(X) = \sum x_i$ is a sufficient statistic. 


\section{CB2 6.1}

$X$ is one observation from $N(0, \sigma^2)$. Is $|X|$ a sufficient statistic?

WE can ignore the front coefficient that only depends on $\sigma^2$: 

$X = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x - \mu)^2}{2\sigma^2}} \text{ with } \mu = 0 \implies X = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2}{2\sigma^2}}$

Because the mean is 0, that leaves only $x^2$ in the numerator of the exponent, which we can also rewrite as $|x|^2$, thus:

$X = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{|x|^2}{2\sigma^2}} = g(T(X), \sigma^2)h(x), h(x) = 1$

Because we can factor it, that means $T(X) = |X|$ is a sufficient statistic (though I think that only holds when $\mu = 0$).


\section{CB2 6.2}

$f_{X_i}(x|\theta) = \begin{cases} e^{i\theta - x} & x \ge i\theta \\ 0 x < i\theta \end{cases}$

Prove that $T = min_i(\frac{X_i}{i})$ is a suffucient statistic for $\theta$

$f(\statvec{x}|\theta) = \prod e^{i\theta - x}I_{(x_i \ge i\theta)}$

For the indicator to hold, we need all $x_i \ge i\theta$, which means we just need the $min(x_i) \ge \theta$, so we can plug in $min(x_i)$ in that portion:

$= e^{i\theta - \sum x_i} I_{x_{i_{(1)} \ge i\theta}} = \underline{[e^{i\theta}I_{x_{i_{(1)} \ge i\theta}}] e^{-\sum x_i}} = g(T(X), \theta)h(x)$

\section{CB2 6.6}

$X \sim Gamma(\alpha, \beta)$. Find a 2D sufficient statistic

Gamma is an exponential family, so we can just put it into exponential family form:

$L(\alpha, \beta | X) \propto \prod x_i^{\alpha - 1}e^{-\beta x_i} = \prod e^{(\alpha - 1)ln(x_i) -\beta x_i} = e^{(\alpha - 1)\sum ln(x_i) - \beta \sum x_i}$

The sufficient statistics are all the $T_i(X)$ in the exponent, so the sufficient statistic is:

$T(X) = (\sum x_i, \sum ln(x_i))$

\section{CB2 6.13}

$X_1, X_2 \iiddist \alpha x^{\alpha - 1}e^{-x^{\alpha}}, x > 0, \alpha > 0$. Show that $\frac{ln(X_1)}{ln(X_2)}$ is ancillary. 

Let $y = ln(x) \implies f(y|\alpha) = \alpha (e^y)^{\alpha-1}e^{-(e^y)^{\alpha}}e^y = \alpha e^{\alpha y - y + y - e^{-y\alpha}} = \alpha e^{\alpha y - e^{-y\alpha}}$

This can be rewritten as a scale-family: $\frac{1}{1/\alpha}e^{\frac{y}{1/\alpha} - e^{(-\frac{y}{1/\alpha})}}$

This is a scale transformation of some variable Z which is not dependent on $\alpha$ (ie. $Z \sim e^{z - e^{-z}})$

Thus: $\frac{ln(X_1)}{ln(X_2)} = \frac{Y_1}{Y_2} = \frac{(1/\alpha)Z_1}{(1/\alpha)Z_2} = \frac{Z_1}{Z_2}$. This does not depend on $\alpha$, thus $\frac{ln(X_1)}{ln(X_2)}$ is an ancillary statistic. 

\section{6}

$X \sim Pois(\lambda)$. Cannot use 6.2.25, the theorem of completeness for exponential families. 

i) Show that $T(X) = \sum_{i=1}^n X_i$ is a complete statistic when $\lambda > 0$

FINISH MEEEEE


ii) Show that for any integer $r > 0$: $\EX[X_i (X_i - 1)(X_i - 2)...(X_i - r +1)] = \lambda^r$

$\EX[X_i (X_i - 1)(X_i - 2)...(X_i - r +1)] = \EX[X_i]\EX[X_i - 1]\EX[X_1 - 2]...\EX[X_i - r + 1] = \EX[X_i](\EX[X_i] - 1)(\EX[X_i] - 2)...(\EX[X_i] - r + 1) = \lambda(\lambda - 1)(\lambda - 2)...(\lambda - r + 1) = \frac{\lambda!}{(\lambda - r)!}$

FINISH MEEEEEE


iii) Restrict $\lambda \in {1, 3}$. Show that the $T(X)$ from part (i) is not a complete statistic. 

FINISH MEEEEE

\section{CB2 6.15}

FINISH MEEEEE

\section{CB2 6.22}

$f(x |\theta) = \theta x^{\theta - 1}$

a) Is $\sum X_i$ a sufficient statistic?

$L(\theta|x) = \prod \theta x_i^{\theta-1} = \theta^n \prod e^{(\theta - 1)ln(x_i)} = \theta^n e^{(\theta - 1)\sum ln(x_i)}$

$\sum ln(x_i)$ is sufficient, but there is no direct linear transformation to $\sum x_i$, meaning $\sum x_i$ is not sufficient. 

b) FINISH MEEEEE


\end{document}