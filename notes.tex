%\documentclass[twocolumn]{article}\
\documentclass{article}
\usepackage{amsmath, amssymb, cancel, mathtools, bm}
\usepackage[left=.5in, right=.5in, top=1in, bottom=1in]{geometry}
\usepackage[most]{tcolorbox}
\usepackage[skip=1em,indent=0pt]{parskip}
\setlength{\parindent}{0pt}
\newcommand{\statvec}[1]{\underset{\sim}{\bm{#1}}} % Vector symbol (tilde under vector)
\DeclareMathOperator{\EX}{\mathbb{E}} % Expected Value symbol
\newcommand{\indep}{\perp\!\!\!\!\perp} % Independence symbol
\newcommand{\real}{\mathbb{R}} % Simplified 'Reals' indicator
\newcommand{\pto}{\overset{P}{\to}}
\newcommand{\asto}{\overset{a.s.}{\to}}
\newcommand{\rthto}{\overset{L^r}{\to}}
\newcommand{\dto}{\overset{D}{\to}}
% Force all aggregate symbols to always put values above/below
\let\Oldint=\int
\let\Oldsum=\sum
\let\Oldprod=\prod
\let\Oldbigcup=\bigcup
\let\Oldbigcap=\bigcap
\let\Oldlim=\lim
\renewcommand{\int}{\Oldint\limits} 
\renewcommand{\sum}{\Oldsum\limits} 
\renewcommand{\prod}{\Oldprod\limits} 
\renewcommand{\bigcup}{\Oldbigcup\limits} 
\renewcommand{\bigcap}{\Oldbigcap\limits} 
\renewcommand{\lim}{\Oldlim\limits} 
\newcommand{\infint}{\int_{-\infty}^{\infty}}
\newcommand{\iiddist}{\overset{\mathrm{iid}}{\sim}}


\begin{document}

\tableofcontents

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Jan 8}

\subsection{Point Estimation}
\underline{Definition}: A point estimator is any scalar (or vector) -valued function of the sample. 
$(x_1, ... x_n) \sim f(x|\theta)$

A point estimator for $\tau(\theta)$ is a statistic $T(x)$ with the purpose of approximating $\tau(\theta)$

\subsection{Method of Moments}
The k-th moment of a r.v. $X$ is $\mu_k(\theta) = \EX_{\theta} (X^k) = \int_{\mathbb{X}} x^k f(x|\theta)dx$

Given an iid sample $x_1 ... x_n \iiddist f(x|\theta)$ we have sample moments: $\hat{\mu}_k = \frac{1}{n}\sum_{i=1}^{n} x_i^k$

Suppose $\theta \in \Theta \in \real^P$, and that $\mu_k(\theta)$ exists and is fininte for k = 1, ..., p.

\underline{Definition}: The method of moments estimator of $\theta$ is the solution to the system of equations:
\begin{align}
\mu_1(\theta) &= \hat{\mu}_1 \\
&. \\
&. \\
\mu_p(\theta) &= \hat{\mu}_p
\end{align}
We call it $\hat{\theta}_{MM}$

\underline{Example}: $X_i \iiddist Beta(\alpha, \beta), \theta = (\alpha, \beta)$

$\mu_1(\theta) = \frac{\alpha}{\alpha + \beta}$

$\mu_2(\theta) = Var_{\theta}(x_1) + \left(\frac{\alpha}{\alpha + \beta}\right)^2 = \frac{\alpha\beta}{(\alpha+\beta)^2 (\alpha + \beta + 1)} + \frac{\alpha^2}{(\alpha + \beta)^2}$

$\frac{\alpha}{\alpha + \beta} = \hat{\mu}_1, \frac{\alpha\beta}{(\alpha+\beta)^2 (\alpha + \beta + 1)} + \frac{\alpha^2}{(\alpha + \beta)^2} = \hat{\mu}_2$

$\beta = \left(\frac{1 - \hat{\mu}_1}{\hat{\mu}_1}\right)\alpha$ from the first equation. Plug this into the second expression and solve: 

$\hat{\alpha}_{MM} = \hat{\mu}_1\left[\frac{\hat{\mu}_1(1-\hat{\mu}_1)}{\hat{\mu}_2 - \hat{\mu}_1^2} - 1 \right]$

$\implies \hat{\beta}_{MM} = \frac{(1-\hat{\mu}_1)}{\hat{\mu}_1}\hat{\alpha}_{MM}$

\underline{Example}: $x_i \iiddist N(\mu, \sigma^2)$

$\mu_1(\theta) = \hat{\mu}_1, \mu_1(\theta) = \mu, \implies \hat{\mu}_{MM} = \mu$

$\mu_2(\theta) = Var_{\theta}(x_1) + \mu_1^2 = \sigma^2 + \mu^2 = \hat{\mu}_2$

$\sigma^2_{MM} = \hat{\mu}_2 - \hat{\mu}_1^2 = \frac{n-1}{n}s^2$

Note: If $\mu, \sigma^2$ are the mean/variance of any family, their MM estimators are $\overline{X}, \frac{n-1}{n}s^2$.

For families where parameters are not just the mean and variance, you can find it via two ways: use mean/variance in MM, then calculate parameters, or use parameters in MM then calculate mean/variance. Both yield same results. 

Fact: MM estimators are invariant of re-parameterizations.

Let $\eta = \eta(\theta)$ be a 1:1 mapping (intervtible). Then, $\hat{\eta}_{MM} = \eta(\hat{\theta}_{MM})$

Let's say $x_i \iiddist N(\mu, \sigma^2)$ and we want to estimate $\tau(\theta) = \frac{\mu}{\sigma}$. This isn't 1:1. What can we do? We can do the 'Transformations' method and create a second value $\tau_2$, etc. We can also just plug in the estimators. 

\underline{Definition}: The MM estimator for a parametric function $\tau(\theta)$ is just $\hat{\tau}_{MM}(\theta) = \tau(\hat(\theta)_{MM})$

\underline{Properties}:
\begin{itemize}
    \item MM equations may have a unique solution, no solution, or many solutions
    \item Often, MM estimators are used as initial values for another estimation technique (ie. a root finding method)
    \item Why should it work? Let's say $\theta^*$ is the true value of $\theta$. Then Law of Large Numbers says: $\hat{\mu}_k \pto \mu_k(\theta*)$. Then we are solving $\mu_k(\theta) = \hat{\mu}_k \approx \mu_k(\theta^*)$
\end{itemize}



\end{document}